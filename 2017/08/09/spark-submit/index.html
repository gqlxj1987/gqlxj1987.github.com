<!DOCTYPE html><html lang="zh-CN"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v4.16.19"><title>Blog</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&family=JetBrains+Mono&display=swap" rel="stylesheet"><style>:root{--primary: #2563eb;--text-main: #1f2937;--text-muted: #6b7280;--bg-main: #ffffff;--bg-secondary: #f9fafb;--border: #e5e7eb;--max-width: 800px;--font-sans: "Inter", system-ui, -apple-system, sans-serif;--font-mono: "JetBrains Mono", monospace}*{margin:0;padding:0;box-sizing:border-box}body{font-family:var(--font-sans);color:var(--text-main);background-color:var(--bg-main);line-height:1.6;-webkit-font-smoothing:antialiased}.app-container{display:flex;flex-direction:column;min-height:100vh}.navbar{border-bottom:1px solid var(--border);position:sticky;top:0;background:#fffc;backdrop-filter:blur(8px);z-index:100}.nav-content{max-width:var(--max-width);margin:0 auto;padding:1rem;display:flex;justify-content:space-between;align-items:center}.logo{text-decoration:none;color:var(--text-main);font-weight:800;font-size:1.5rem}.nav-links{list-style:none;display:flex;gap:1.5rem}.nav-links a{text-decoration:none;color:var(--text-muted);font-size:.95rem;font-weight:500;transition:color .2s}.nav-links a:hover{color:var(--primary)}.main-content{flex:1;max-width:var(--max-width);margin:0 auto;width:100%;padding:3rem 1rem}.footer{border-top:1px solid var(--border);background:var(--bg-secondary);padding:2rem 1rem;margin-top:4rem}.footer-content{max-width:var(--max-width);margin:0 auto;text-align:center;color:var(--text-muted);font-size:.9rem}.post-content h1,.post-content h2,.post-content h3{margin-top:2.5rem;margin-bottom:1rem;line-height:1.3}.post-content p{margin-bottom:1.5rem}.post-content img{max-width:100%;height:auto;border-radius:8px;margin:2rem 0}.post-content pre{border-radius:8px;padding:1.25rem;margin-bottom:2rem;font-family:var(--font-mono);font-size:.9rem;overflow-x:auto}.post-content code:not(pre code){background:var(--bg-secondary);padding:.2rem .4rem;border-radius:4px;font-family:var(--font-mono);font-size:.9em}.post-content blockquote{border-left:4px solid var(--primary);padding-left:1.5rem;font-style:italic;color:var(--text-muted);margin:2rem 0}.post-content ul,.post-content ol{margin-bottom:1.5rem;padding-left:1.5rem}.post-content li{margin-bottom:.5rem}
.post-header[data-astro-cid-rl3favg5]{margin-bottom:4rem;text-align:center}.post-date[data-astro-cid-rl3favg5]{display:block;font-size:.875rem;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);font-weight:600;margin-bottom:1rem}.post-title[data-astro-cid-rl3favg5]{font-size:3rem;font-weight:800;line-height:1.1;margin-bottom:1.5rem;letter-spacing:-.025em}.post-meta[data-astro-cid-rl3favg5]{display:flex;justify-content:center;gap:1.5rem;font-size:.95rem;color:var(--text-muted)}.tag[data-astro-cid-rl3favg5]{color:var(--primary);margin-right:.5rem}@media (max-width: 640px){.post-title[data-astro-cid-rl3favg5]{font-size:2rem}}
</style></head> <body> <div class="app-container"> <header class="navbar"> <div class="nav-content"> <a href="/" class="logo"> <span class="logo-text">Blog</span> </a> <nav> <ul class="nav-links"> <li><a href="/">Home</a></li> <li><a href="/archive">Archive</a></li> <li><a href="/about">About</a></li> </ul> </nav> </div> </header> <main class="main-content">  <article class="post" data-astro-cid-rl3favg5> <header class="post-header" data-astro-cid-rl3favg5> <time class="post-date" data-astro-cid-rl3favg5> August 9, 2017 </time> <h1 class="post-title" data-astro-cid-rl3favg5></h1> <div class="post-meta" data-astro-cid-rl3favg5>   </div> </header> <div class="post-content" data-astro-cid-rl3favg5> <h2 id="title-spark-submit-参数分析date-2017-08-09-075400categories-sparktags-spark">title: Spark Submit 参数分析
date: 2017-08-09 07:54:00
categories: spark
tags: [spark]</h2>
<p><a href="https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application?utm_campaign=CodeTengu&#x26;utm_medium=email&#x26;utm_source=CodeTengu_99">Ever wondered how to configure —num-executors, —executor-memory and —execuor-cores spark config params for your cluster?</a></p>
<ul>
<li>
<p>Lil bit theory: Let’s see some key recommendations that will help understand it better</p>
</li>
<li>
<p>Hands on: Next, we’ll take an example cluster and come up with recommended numbers to these spark params</p>
</li>
</ul>
<p>Lil bit theory:</p>
<ul>
<li>Hadoop/Yarn/OS Deamons:</li>
</ul>
<p>serveral daemons that’ll run in the background like NameNode, Secondary NameNode, DataNode, JobTracker and TaskTracker</p>
<p>num-executors, we need to make sure that we leave aside enough cores (~1 core per node)</p>
<ul>
<li>Yarn ApplicationMaster (AM)</li>
</ul>
<p>If we are running spark on yarn, then we need to budget in the resources that AM would need (~1024MB and 1 Executor)</p>
<ul>
<li>
<p>HDFS Throughput</p>
</li>
<li>
<p>MemoryOverhead</p>
</li>
</ul>
<blockquote>
<p>Full memory requested to yarn per executor =<code>spark-executor-memory + spark.yarn.executor.memoryOverhead</code></p>
</blockquote>
<blockquote>
<p>spark.yarn.executor.memoryOverhead = <code>Max(384MB, 7% of spark.executor-memory)</code></p>
</blockquote>
<p>So, if we request 20GB per executor, AM will actually get 20GB + memoryOverhead = 20 + 7% of 20GB = ~23GB memory for us.</p>
<p>tips:</p>
<p>Running tiny executors (with a single core and just enough memory needed to run a single task, for example) throws away the benefits that come from running multiple tasks in a single JVM.</p>
<p>相关的配置:</p>
<blockquote>
<p><strong>Cluster Config:</strong>
10 Nodes
16 cores per Node
64GB RAM per Node</p>
</blockquote>
<ul>
<li>First Approach: Tiny executors [One Executor per core]:</li>
</ul>
<blockquote>
<ul>
<li><code>--num-executors</code> = <code>In this approach, we'll assign one executor per core</code></li>
</ul>
</blockquote>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="shell"><code><span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> `</span><span style="color:#B392F0">total-cores-in-cluster</span><span style="color:#9ECBFF">`</span></span>
<span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> `</span><span style="color:#B392F0">num-cores-per-node</span><span style="color:#79B8FF"> *</span><span style="color:#9ECBFF"> total-nodes-in-cluster`</span><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#9ECBFF">=</span><span style="color:#79B8FF"> 16</span><span style="color:#9ECBFF"> x</span><span style="color:#79B8FF"> 10</span><span style="color:#9ECBFF"> =</span><span style="color:#79B8FF"> 160</span></span>
<span class="line"></span></code></pre>
<ul>
<li><code>--executor-cores</code> = 1 (one executor per core)</li>
<li><code>--executor-memory</code> = <code>amount of memory per executor</code>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="shell"><code><span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> `</span><span style="color:#B392F0">mem-per-node/num-executors-per-node</span><span style="color:#9ECBFF">`</span></span>
<span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> 64GB/16</span><span style="color:#9ECBFF"> =</span><span style="color:#9ECBFF"> 4GB</span></span>
<span class="line"></span></code></pre>
</li>
</ul>
<p>Not Good!</p>
<ul>
<li>Second Approach: Fat executors (One Executor per node):</li>
</ul>
<blockquote>
<ul>
<li><code>--num-executors</code> = <code>In this approach, we'll assign one executor per node</code></li>
</ul>
</blockquote>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="shell"><code><span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> `</span><span style="color:#B392F0">total-nodes-in-cluster</span><span style="color:#9ECBFF">`</span></span>
<span class="line"><span style="color:#9ECBFF">=</span><span style="color:#79B8FF"> 10</span></span>
<span class="line"></span></code></pre>
<ul>
<li>
<p><code>--executor-cores</code> = <code>one executor per node means all the cores of the node are assigned to one executor</code></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="shell"><code><span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> `</span><span style="color:#B392F0">total-cores-in-a-node</span><span style="color:#9ECBFF">`</span></span>
<span class="line"><span style="color:#9ECBFF">=</span><span style="color:#79B8FF"> 16</span></span>
<span class="line"></span></code></pre>
</li>
<li>
<p><code>--executor-memory</code> = <code>amount of memory per executor</code></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="shell"><code><span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> `</span><span style="color:#B392F0">mem-per-node/num-executors-per-node</span><span style="color:#9ECBFF">`</span></span>
<span class="line"><span style="color:#9ECBFF">=</span><span style="color:#9ECBFF"> 64GB/1</span><span style="color:#9ECBFF"> =</span><span style="color:#9ECBFF"> 64GB</span></span>
<span class="line"></span></code></pre>
<p>​<br>
Not Good for HDFS throughput</p>
</li>
</ul>
<ul>
<li>
<p>Third Approach: Balance between Fat (vs) Tiny</p>
<ul>
<li>Based on the recommendations mentioned above, Let’s assign 5 core per executors => —executor-cores = 5 (for good HDFS throughput)</li>
<li>Leave 1 core per node for Hadoop/Yarn daemons => Num cores available per node = 16-1 = 15</li>
<li>So, Total available of cores in cluster = 15 x 10 = 150</li>
<li>Number of available executors = (total cores/num-cores-per-executor) = 150/5 = 30</li>
<li>Leaving 1 executor for ApplicationManager => —num-executors = 29</li>
<li>Number of executors per node = 30/10 = 3</li>
<li>Memory per executor = 64GB/3 = 21GB</li>
<li>Counting off heap overhead = 7% of 21GB = 3GB. So, actual —executor-memory = 21 - 3 = 18GB</li>
</ul>
</li>
</ul>
<p>recommended config is: 29 executors, 18GB memory each and 5 cores each!!</p> </div> </article>  </main> <footer class="footer"> <div class="footer-content"> <p>&copy; 2026 Liu Weifeng. Built with Astro.</p> </div> </footer> </div> </body></html> 