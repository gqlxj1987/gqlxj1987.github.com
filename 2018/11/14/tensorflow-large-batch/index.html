<!DOCTYPE html><html lang="zh-CN"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v4.16.19"><meta name="description" content="Blog post"><title>Untitled</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Lora:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet"><link rel="stylesheet" href="/_astro/_title_.8kzbYMPX.css"><script type="module">const e=()=>{document.querySelectorAll(".animate").forEach((t,s)=>{setTimeout(()=>{t.classList.add("show")},s*150)})};e();document.addEventListener("astro:after-swap",e);
</script></head> <body> <header> <div class="mx-auto max-w-screen-sm px-5">  <div class="flex flex-wrap gap-y-2 justify-between"> <a href="/" class="font-semibold hover:underline underline-offset-2">
Blog
</a> <nav class="flex gap-1">  <a href="/archive" class="hover:underline underline-offset-2"> Archive </a> <span>/</span> <a href="/about" class="hover:underline underline-offset-2"> About </a>  </nav> </div>  </div> </header> <main>  <div class="mx-auto max-w-screen-sm px-5">  <div class="animate"> <a href="/archive" class="hover:underline underline-offset-2">
← Back to archive
</a> </div> <div class="space-y-1 my-10"> <div class="animate flex items-center gap-1.5"> <div class="font-base text-sm"> <time datetime="2018-11-14T00:00:00.000Z"> November 14, 2018 </time> </div> </div> <div class="animate text-2xl font-semibold text-black dark:text-white">  </div>  </div> <article class="animate"> <p>title: Tensorflow Large batch
date: 2018-11-14 22:53:58
categories: tensorflow</p>
<h2 id="tags-tensorflow">tags: [tensorflow]</h2>
<p>GPU不够的情况下，</p>
<p>在TensorFlow上，我们可以比较方便地定制一个optimizer来实现这种操作，封装一下实际的optimizer，实际上做梯度累加和延迟更新两部就好了。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> LazyUpdateOptimizer</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">tf</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">train</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Optimizer</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, optimizer, batch_size</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E1E4E8">                 use_locking</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">, name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"LazyUpdateOptimizer"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">        tf.train.Optimizer.</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">use_locking</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">use_locking, </span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">name)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._name </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> name</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._batch_size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> batch_size</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._grad_cache </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {}</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._optimizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> optimizer</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._vars </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> []</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.variable_scope(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._name):</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">._batch_count_variable </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> \</span></span>
<span class="line"><span style="color:#E1E4E8">                tf.get_variable(</span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"batch_count"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">                                shape</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[],</span></span>
<span class="line"><span style="color:#FFAB70">                                dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tf.int64,</span></span>
<span class="line"><span style="color:#FFAB70">                                initializer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tf.constant_initializer(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_size),</span></span>
<span class="line"><span style="color:#FFAB70">                                collections</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[tf.GraphKeys.</span><span style="color:#79B8FF">LOCAL_VARIABLES</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">._vars.append(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">property</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> optimizer</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._optimizer</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">property</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> name</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._name</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">property</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> batch_size</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._batch_size</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> get_initializer</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> tf.group([_.initializer </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._vars])</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> apply_gradients</span><span style="color:#E1E4E8">(self, grads_and_vars, global_step</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, name</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">        scope_name </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._name</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> name </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">            scope_name </span><span style="color:#F97583">+=</span><span style="color:#9ECBFF"> "_"</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> name</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">        cached_grads </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> []</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> grad, var </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> grads_and_vars:</span></span>
<span class="line"><span style="color:#F97583">            if</span><span style="color:#E1E4E8"> grad </span><span style="color:#F97583">is</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">                continue</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            if</span><span style="color:#E1E4E8"> var </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#F97583"> and</span><span style="color:#E1E4E8"> var </span><span style="color:#F97583">not</span><span style="color:#F97583"> in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._grad_cache:</span></span>
<span class="line"><span style="color:#F97583">                with</span><span style="color:#E1E4E8"> tf.variable_scope(scope_name):</span></span>
<span class="line"><span style="color:#F97583">                    with</span><span style="color:#E1E4E8"> tf.colocate_with(var):</span></span>
<span class="line"><span style="color:#E1E4E8">                        cached_grad </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.get_variable(</span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">var.name.split(</span><span style="color:#9ECBFF">":"</span><span style="color:#E1E4E8">)[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">+</span><span style="color:#9ECBFF"> "_grad_cache"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">var.dtype,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      shape</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">var.shape,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      initializer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tf.zeros_initializer(),</span></span>
<span class="line"><span style="color:#FFAB70">                                                      trainable</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      collections</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[tf.GraphKeys.</span><span style="color:#79B8FF">LOCAL_VARIABLES</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">                        self</span><span style="color:#E1E4E8">._vars.append(cached_grad)</span></span>
<span class="line"><span style="color:#79B8FF">                self</span><span style="color:#E1E4E8">._grad_cache[var] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cached_grad</span></span>
<span class="line"><span style="color:#E1E4E8">            cached_grads.append(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._grad_cache[var])</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.name_scope(scope_name):</span></span>
<span class="line"><span style="color:#E1E4E8">            cache_gradients_op </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.__cache_gradients(grads_and_vars, cached_grads)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            with</span><span style="color:#E1E4E8"> tf.control_dependencies([cache_gradients_op]):</span></span>
<span class="line"><span style="color:#E1E4E8">                apply_op </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.cond(</span></span>
<span class="line"><span style="color:#E1E4E8">                    tf.equal(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#FFAB70">                    true_fn</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.__actual_apply_gradients(grads_and_vars, </span><span style="color:#FFAB70">global_step</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">global_step),</span></span>
<span class="line"><span style="color:#FFAB70">                    false_fn</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8">: tf.no_op())</span></span>
<span class="line"><span style="color:#F97583">                with</span><span style="color:#E1E4E8"> tf.control_dependencies([apply_op]):</span></span>
<span class="line"><span style="color:#F97583">                    return</span><span style="color:#E1E4E8"> tf.no_op()</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> __cache_gradients</span><span style="color:#E1E4E8">(self, grads_and_vars, cached_grads):</span></span>
<span class="line"><span style="color:#E1E4E8">        update_ops </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> []</span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.name_scope(</span><span style="color:#9ECBFF">"cache_grad"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> (grad, var), cached_grad </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> itertools.izip(grads_and_vars, cached_grads):</span></span>
<span class="line"><span style="color:#F97583">                with</span><span style="color:#E1E4E8"> tf.colocate_with(cached_grad):</span></span>
<span class="line"><span style="color:#F97583">                    if</span><span style="color:#79B8FF"> isinstance</span><span style="color:#E1E4E8">(grad, tf.Tensor):</span></span>
<span class="line"><span style="color:#E1E4E8">                        update_op </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.assign_add(cached_grad, grad)</span></span>
<span class="line"><span style="color:#F97583">                    elif</span><span style="color:#79B8FF"> isinstance</span><span style="color:#E1E4E8">(grad, tf.IndexedSlices):</span></span>
<span class="line"><span style="color:#E1E4E8">                        update_op </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.scatter_add(cached_grad, grad.indices, grad.values)</span></span>
<span class="line"><span style="color:#F97583">                    else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">                        continue</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">                update_ops.append(update_op)</span></span>
<span class="line"><span style="color:#F97583">            with</span><span style="color:#E1E4E8"> tf.control_dependencies([tf.group(update_ops, </span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"record_gradients"</span><span style="color:#E1E4E8">)]):</span></span>
<span class="line"><span style="color:#F97583">                return</span><span style="color:#E1E4E8"> tf.assign_sub(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> __actual_apply_gradients</span><span style="color:#E1E4E8">(self, grads_and_vars, global_step</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">        actual_grads_and_vars </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._grad_cache[var], var) </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> grad, var </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> grads_and_vars </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> grad </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">        apply_op </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._optimizer.apply_gradients(actual_grads_and_vars, </span><span style="color:#FFAB70">global_step</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">global_step)</span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.control_dependencies([apply_op]):</span></span>
<span class="line"><span style="color:#E1E4E8">            reset_ops </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [tf.assign(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_size)]</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> grad, var </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> actual_grads_and_vars:</span></span>
<span class="line"><span style="color:#E1E4E8">                reset_ops.append(tf.assign(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._grad_cache[var], tf.zeros_like(var)))</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            with</span><span style="color:#E1E4E8"> tf.control_dependencies(reset_ops):</span></span>
<span class="line"><span style="color:#F97583">                return</span><span style="color:#E1E4E8"> tf.no_op()</span></span>
<span class="line"></span></code></pre> </article>  </div>  </main> <footer class="animate" data-astro-cid-sz7xmlte> <div class="mx-auto max-w-screen-sm px-5">  <div class="flex justify-between items-center" data-astro-cid-sz7xmlte> <div data-astro-cid-sz7xmlte>
&copy; 2026 | Blog
</div> <div class="flex flex-wrap gap-1 items-center" data-astro-cid-sz7xmlte> <button id="light-theme-button" aria-label="Light theme" class="group size-8 flex items-center justify-center rounded-full" data-astro-cid-sz7xmlte> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="group-hover:stroke-black group-hover:dark:stroke-white transition-colors duration-300 ease-in-out" data-astro-cid-sz7xmlte> <circle cx="12" cy="12" r="5" data-astro-cid-sz7xmlte></circle> <line x1="12" y1="1" x2="12" y2="3" data-astro-cid-sz7xmlte></line> <line x1="12" y1="21" x2="12" y2="23" data-astro-cid-sz7xmlte></line> <line x1="4.22" y1="4.22" x2="5.64" y2="5.64" data-astro-cid-sz7xmlte></line> <line x1="18.36" y1="18.36" x2="19.78" y2="19.78" data-astro-cid-sz7xmlte></line> <line x1="1" y1="12" x2="3" y2="12" data-astro-cid-sz7xmlte></line> <line x1="21" y1="12" x2="23" y2="12" data-astro-cid-sz7xmlte></line> <line x1="4.22" y1="19.78" x2="5.64" y2="18.36" data-astro-cid-sz7xmlte></line> <line x1="18.36" y1="5.64" x2="19.78" y2="4.22" data-astro-cid-sz7xmlte></line> </svg> </button> <button id="dark-theme-button" aria-label="Dark theme" class="group size-8 flex items-center justify-center rounded-full" data-astro-cid-sz7xmlte> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="group-hover:stroke-black group-hover:dark:stroke-white transition-colors duration-300 ease-in-out" data-astro-cid-sz7xmlte> <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" data-astro-cid-sz7xmlte></path> </svg> </button> <button id="system-theme-button" aria-label="System theme" class="group size-8 flex items-center justify-center rounded-full" data-astro-cid-sz7xmlte> <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="group-hover:stroke-black group-hover:dark:stroke-white transition-colors duration-300 ease-in-out" data-astro-cid-sz7xmlte> <rect x="2" y="3" width="20" height="14" rx="2" ry="2" data-astro-cid-sz7xmlte></rect> <line x1="8" y1="21" x2="16" y2="21" data-astro-cid-sz7xmlte></line> <line x1="12" y1="17" x2="12" y2="21" data-astro-cid-sz7xmlte></line> </svg> </button> </div> </div>  </div> </footer>  <script>
  function setTheme(theme) {
    document.documentElement.classList.remove("light", "dark", "system");
    
    if (theme === "system") {
      document.documentElement.classList.add("system");
      const systemTheme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light";
      document.documentElement.classList.add(systemTheme);
    } else {
      document.documentElement.classList.add(theme);
    }
    
    localStorage.setItem("theme", theme);
  }

  document.getElementById("light-theme-button")?.addEventListener("click", () => setTheme("light"));
  document.getElementById("dark-theme-button")?.addEventListener("click", () => setTheme("dark"));
  document.getElementById("system-theme-button")?.addEventListener("click", () => setTheme("system"));

  const savedTheme = localStorage.getItem("theme") || "system";
  setTheme(savedTheme);
</script>  </body> </html>