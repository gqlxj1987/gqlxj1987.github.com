<!DOCTYPE html><html lang="zh-CN"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v4.16.19"><title>Blog</title><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&family=JetBrains+Mono&display=swap" rel="stylesheet"><style>:root{--primary: #2563eb;--text-main: #1f2937;--text-muted: #6b7280;--bg-main: #ffffff;--bg-secondary: #f9fafb;--border: #e5e7eb;--max-width: 800px;--font-sans: "Inter", system-ui, -apple-system, sans-serif;--font-mono: "JetBrains Mono", monospace}*{margin:0;padding:0;box-sizing:border-box}body{font-family:var(--font-sans);color:var(--text-main);background-color:var(--bg-main);line-height:1.6;-webkit-font-smoothing:antialiased}.app-container{display:flex;flex-direction:column;min-height:100vh}.navbar{border-bottom:1px solid var(--border);position:sticky;top:0;background:#fffc;backdrop-filter:blur(8px);z-index:100}.nav-content{max-width:var(--max-width);margin:0 auto;padding:1rem;display:flex;justify-content:space-between;align-items:center}.logo{text-decoration:none;color:var(--text-main);font-weight:800;font-size:1.5rem}.nav-links{list-style:none;display:flex;gap:1.5rem}.nav-links a{text-decoration:none;color:var(--text-muted);font-size:.95rem;font-weight:500;transition:color .2s}.nav-links a:hover{color:var(--primary)}.main-content{flex:1;max-width:var(--max-width);margin:0 auto;width:100%;padding:3rem 1rem}.footer{border-top:1px solid var(--border);background:var(--bg-secondary);padding:2rem 1rem;margin-top:4rem}.footer-content{max-width:var(--max-width);margin:0 auto;text-align:center;color:var(--text-muted);font-size:.9rem}.post-content h1,.post-content h2,.post-content h3{margin-top:2.5rem;margin-bottom:1rem;line-height:1.3}.post-content p{margin-bottom:1.5rem}.post-content img{max-width:100%;height:auto;border-radius:8px;margin:2rem 0}.post-content pre{border-radius:8px;padding:1.25rem;margin-bottom:2rem;font-family:var(--font-mono);font-size:.9rem;overflow-x:auto}.post-content code:not(pre code){background:var(--bg-secondary);padding:.2rem .4rem;border-radius:4px;font-family:var(--font-mono);font-size:.9em}.post-content blockquote{border-left:4px solid var(--primary);padding-left:1.5rem;font-style:italic;color:var(--text-muted);margin:2rem 0}.post-content ul,.post-content ol{margin-bottom:1.5rem;padding-left:1.5rem}.post-content li{margin-bottom:.5rem}
.post-header[data-astro-cid-rl3favg5]{margin-bottom:4rem;text-align:center}.post-date[data-astro-cid-rl3favg5]{display:block;font-size:.875rem;text-transform:uppercase;letter-spacing:.05em;color:var(--text-muted);font-weight:600;margin-bottom:1rem}.post-title[data-astro-cid-rl3favg5]{font-size:3rem;font-weight:800;line-height:1.1;margin-bottom:1.5rem;letter-spacing:-.025em}.post-meta[data-astro-cid-rl3favg5]{display:flex;justify-content:center;gap:1.5rem;font-size:.95rem;color:var(--text-muted)}.tag[data-astro-cid-rl3favg5]{color:var(--primary);margin-right:.5rem}@media (max-width: 640px){.post-title[data-astro-cid-rl3favg5]{font-size:2rem}}
</style></head> <body> <div class="app-container"> <header class="navbar"> <div class="nav-content"> <a href="/" class="logo"> <span class="logo-text">Blog</span> </a> <nav> <ul class="nav-links"> <li><a href="/">Home</a></li> <li><a href="/archive">Archive</a></li> <li><a href="/about">About</a></li> </ul> </nav> </div> </header> <main class="main-content">  <article class="post" data-astro-cid-rl3favg5> <header class="post-header" data-astro-cid-rl3favg5> <time class="post-date" data-astro-cid-rl3favg5> November 14, 2018 </time> <h1 class="post-title" data-astro-cid-rl3favg5></h1> <div class="post-meta" data-astro-cid-rl3favg5>   </div> </header> <div class="post-content" data-astro-cid-rl3favg5> <p>title: Tensorflow Large batch
date: 2018-11-14 22:53:58
categories: tensorflow</p>
<h2 id="tags-tensorflow">tags: [tensorflow]</h2>
<p>GPU不够的情况下，</p>
<p>在TensorFlow上，我们可以比较方便地定制一个optimizer来实现这种操作，封装一下实际的optimizer，实际上做梯度累加和延迟更新两部就好了。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">class</span><span style="color:#B392F0"> LazyUpdateOptimizer</span><span style="color:#E1E4E8">(</span><span style="color:#B392F0">tf</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">train</span><span style="color:#E1E4E8">.</span><span style="color:#B392F0">Optimizer</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#79B8FF"> __init__</span><span style="color:#E1E4E8">(self, optimizer, batch_size</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#E1E4E8">                 use_locking</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">, name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"LazyUpdateOptimizer"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">        tf.train.Optimizer.</span><span style="color:#79B8FF">__init__</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">use_locking</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">use_locking, </span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">name)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._name </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> name</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._batch_size </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> batch_size</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._grad_cache </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> {}</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._optimizer </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> optimizer</span></span>
<span class="line"><span style="color:#79B8FF">        self</span><span style="color:#E1E4E8">._vars </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> []</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.variable_scope(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._name):</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">._batch_count_variable </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> \</span></span>
<span class="line"><span style="color:#E1E4E8">                tf.get_variable(</span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"batch_count"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">                                shape</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[],</span></span>
<span class="line"><span style="color:#FFAB70">                                dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tf.int64,</span></span>
<span class="line"><span style="color:#FFAB70">                                initializer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tf.constant_initializer(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_size),</span></span>
<span class="line"><span style="color:#FFAB70">                                collections</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[tf.GraphKeys.</span><span style="color:#79B8FF">LOCAL_VARIABLES</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">            self</span><span style="color:#E1E4E8">._vars.append(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">property</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> optimizer</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._optimizer</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">property</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> name</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._name</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#B392F0">    @</span><span style="color:#79B8FF">property</span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> batch_size</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._batch_size</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> get_initializer</span><span style="color:#E1E4E8">(self):</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#E1E4E8"> tf.group([_.initializer </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._vars])</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> apply_gradients</span><span style="color:#E1E4E8">(self, grads_and_vars, global_step</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, name</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">        scope_name </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._name</span></span>
<span class="line"><span style="color:#F97583">        if</span><span style="color:#E1E4E8"> name </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">            scope_name </span><span style="color:#F97583">+=</span><span style="color:#9ECBFF"> "_"</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> name</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">        cached_grads </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> []</span></span>
<span class="line"><span style="color:#F97583">        for</span><span style="color:#E1E4E8"> grad, var </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> grads_and_vars:</span></span>
<span class="line"><span style="color:#F97583">            if</span><span style="color:#E1E4E8"> grad </span><span style="color:#F97583">is</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">                continue</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            if</span><span style="color:#E1E4E8"> var </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#F97583"> and</span><span style="color:#E1E4E8"> var </span><span style="color:#F97583">not</span><span style="color:#F97583"> in</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._grad_cache:</span></span>
<span class="line"><span style="color:#F97583">                with</span><span style="color:#E1E4E8"> tf.variable_scope(scope_name):</span></span>
<span class="line"><span style="color:#F97583">                    with</span><span style="color:#E1E4E8"> tf.colocate_with(var):</span></span>
<span class="line"><span style="color:#E1E4E8">                        cached_grad </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.get_variable(</span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">var.name.split(</span><span style="color:#9ECBFF">":"</span><span style="color:#E1E4E8">)[</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">+</span><span style="color:#9ECBFF"> "_grad_cache"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">var.dtype,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      shape</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">var.shape,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      initializer</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tf.zeros_initializer(),</span></span>
<span class="line"><span style="color:#FFAB70">                                                      trainable</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">                                                      collections</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[tf.GraphKeys.</span><span style="color:#79B8FF">LOCAL_VARIABLES</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">                        self</span><span style="color:#E1E4E8">._vars.append(cached_grad)</span></span>
<span class="line"><span style="color:#79B8FF">                self</span><span style="color:#E1E4E8">._grad_cache[var] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> cached_grad</span></span>
<span class="line"><span style="color:#E1E4E8">            cached_grads.append(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._grad_cache[var])</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.name_scope(scope_name):</span></span>
<span class="line"><span style="color:#E1E4E8">            cache_gradients_op </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">.__cache_gradients(grads_and_vars, cached_grads)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            with</span><span style="color:#E1E4E8"> tf.control_dependencies([cache_gradients_op]):</span></span>
<span class="line"><span style="color:#E1E4E8">                apply_op </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.cond(</span></span>
<span class="line"><span style="color:#E1E4E8">                    tf.equal(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable, </span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#FFAB70">                    true_fn</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">.__actual_apply_gradients(grads_and_vars, </span><span style="color:#FFAB70">global_step</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">global_step),</span></span>
<span class="line"><span style="color:#FFAB70">                    false_fn</span><span style="color:#F97583">=lambda</span><span style="color:#E1E4E8">: tf.no_op())</span></span>
<span class="line"><span style="color:#F97583">                with</span><span style="color:#E1E4E8"> tf.control_dependencies([apply_op]):</span></span>
<span class="line"><span style="color:#F97583">                    return</span><span style="color:#E1E4E8"> tf.no_op()</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> __cache_gradients</span><span style="color:#E1E4E8">(self, grads_and_vars, cached_grads):</span></span>
<span class="line"><span style="color:#E1E4E8">        update_ops </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> []</span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.name_scope(</span><span style="color:#9ECBFF">"cache_grad"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> (grad, var), cached_grad </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> itertools.izip(grads_and_vars, cached_grads):</span></span>
<span class="line"><span style="color:#F97583">                with</span><span style="color:#E1E4E8"> tf.colocate_with(cached_grad):</span></span>
<span class="line"><span style="color:#F97583">                    if</span><span style="color:#79B8FF"> isinstance</span><span style="color:#E1E4E8">(grad, tf.Tensor):</span></span>
<span class="line"><span style="color:#E1E4E8">                        update_op </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.assign_add(cached_grad, grad)</span></span>
<span class="line"><span style="color:#F97583">                    elif</span><span style="color:#79B8FF"> isinstance</span><span style="color:#E1E4E8">(grad, tf.IndexedSlices):</span></span>
<span class="line"><span style="color:#E1E4E8">                        update_op </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tf.scatter_add(cached_grad, grad.indices, grad.values)</span></span>
<span class="line"><span style="color:#F97583">                    else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">                        continue</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">                update_ops.append(update_op)</span></span>
<span class="line"><span style="color:#F97583">            with</span><span style="color:#E1E4E8"> tf.control_dependencies([tf.group(update_ops, </span><span style="color:#FFAB70">name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"record_gradients"</span><span style="color:#E1E4E8">)]):</span></span>
<span class="line"><span style="color:#F97583">                return</span><span style="color:#E1E4E8"> tf.assign_sub(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable, </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">    def</span><span style="color:#B392F0"> __actual_apply_gradients</span><span style="color:#E1E4E8">(self, grads_and_vars, global_step</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">        actual_grads_and_vars </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._grad_cache[var], var) </span><span style="color:#F97583">for</span><span style="color:#E1E4E8"> grad, var </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> grads_and_vars </span><span style="color:#F97583">if</span><span style="color:#E1E4E8"> grad </span><span style="color:#F97583">is</span><span style="color:#F97583"> not</span><span style="color:#79B8FF"> None</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#E1E4E8">        apply_op </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> self</span><span style="color:#E1E4E8">._optimizer.apply_gradients(actual_grads_and_vars, </span><span style="color:#FFAB70">global_step</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">global_step)</span></span>
<span class="line"><span style="color:#F97583">        with</span><span style="color:#E1E4E8"> tf.control_dependencies([apply_op]):</span></span>
<span class="line"><span style="color:#E1E4E8">            reset_ops </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [tf.assign(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_count_variable, </span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._batch_size)]</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            for</span><span style="color:#E1E4E8"> grad, var </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> actual_grads_and_vars:</span></span>
<span class="line"><span style="color:#E1E4E8">                reset_ops.append(tf.assign(</span><span style="color:#79B8FF">self</span><span style="color:#E1E4E8">._grad_cache[var], tf.zeros_like(var)))</span></span>
<span class="line"><span style="color:#E1E4E8"> </span></span>
<span class="line"><span style="color:#F97583">            with</span><span style="color:#E1E4E8"> tf.control_dependencies(reset_ops):</span></span>
<span class="line"><span style="color:#F97583">                return</span><span style="color:#E1E4E8"> tf.no_op()</span></span>
<span class="line"></span></code></pre> </div> </article>  </main> <footer class="footer"> <div class="footer-content"> <p>&copy; 2026 Liu Weifeng. Built with Astro.</p> </div> </footer> </div> </body></html> 